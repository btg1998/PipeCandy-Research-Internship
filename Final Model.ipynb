{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to tokenize the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(input):\n",
    "\ttokensBySlash = str(input.encode('utf-8')).split('/')\t#get tokens after splitting by slash\n",
    "\tallTokens = []\n",
    "\tfor i in tokensBySlash:\n",
    "\t\ttokens = str(i).split('-')\t#get tokens after splitting by dash\n",
    "\t\ttokensByDot = []\n",
    "\t\tfor j in range(0,len(tokens)):\n",
    "\t\t\ttempTokens = str(tokens[j]).split('.')\t#get tokens after splitting by dot\n",
    "\t\t\ttokensByDot = tokensByDot + tempTokens\n",
    "\t\tallTokens = allTokens + tokens + tokensByDot\n",
    "\tallTokens = list(set(allTokens))\t#remove redundant tokens\n",
    "\tif 'com' in allTokens:\n",
    "\t\tallTokens.remove('com')\t#removing .com since it occurs a lot of times and it should not be included in our features\n",
    "\tif 'www' in allTokens:\n",
    "\t\tallTokens.remove('www')\t#removing www since it occurs a lot of times and it should not be included in our features\n",
    "\tif 'http' in allTokens:\n",
    "\t\tallTokens.remove('http')\t#removing http since it occurs a lot of times and it should not be included in our features\n",
    "\tif 'https' in allTokens:\n",
    "\t\tallTokens.remove('https')#removing .com since it occurs a lot of times and it should not be included in our features\n",
    "  \n",
    "\treturn allTokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TL():\n",
    "\tallurls = 'Sheet containing URLs for training.csv'\t#Ente the path to our all urls file, I have used the path where the files are in my System.\n",
    "\tallurlscsv = pd.read_csv(allurls,',',error_bad_lines=False)\t#reading file\n",
    "\tallurlsdata = pd.DataFrame(allurlscsv)\t#converting to a dataframe\n",
    "\n",
    "\tallurlsdata = np.array(allurlsdata)\t#converting it into an array\n",
    "\trandom.shuffle(allurlsdata)\t#shuffling\n",
    "\n",
    "\ty = [d[1] for d in allurlsdata]\t#all labels \n",
    "\tcorpus = [d[0] for d in allurlsdata]\t#all urls corresponding to a label (either product or non-product)\n",
    "\tvectorizer = TfidfVectorizer(tokenizer=getTokens)\t#get a vector for each url but use our customized tokenizer which we defined above\n",
    "\tX = vectorizer.fit_transform(corpus)\t#get the X vector\n",
    "\tprint(X)\n",
    "\tX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=108)\t#split into training and testing set 80/20 ratio\n",
    "    \n",
    "    # Here I have used Logistic Regression as the Algo for Classification as it gave me the highest accuracy\n",
    "    # but we can also try various models as our Data Set Evolves \n",
    "\tlgs = LogisticRegression()\t#using logistic regression\n",
    "\tlgs.fit(X_train, y_train)\n",
    "\tprint(lgs.score(X_test, y_test))\t#Printing the accuracy score\n",
    "\treturn vectorizer, lgs # Here we are returning the function used to Vectorize the URL as well as the model to be used for prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.101610365508\n",
      "  (0, 49)\t0.212379651764\n",
      "  (0, 30)\t0.368477001669\n",
      "  (0, 21)\t0.329289696031\n",
      "  (0, 75)\t0.474299510549\n",
      "  (0, 37)\t0.260911562139\n",
      "  (0, 31)\t0.260911562139\n",
      "  (0, 50)\t0.214985188511\n",
      "  (0, 118)\t0.260911562139\n",
      "  (0, 162)\t0.474299510549\n",
      "  (1, 0)\t0.101610365508\n",
      "  (1, 49)\t0.212379651764\n",
      "  (1, 30)\t0.368477001669\n",
      "  (1, 21)\t0.329289696031\n",
      "  (1, 75)\t0.474299510549\n",
      "  (1, 37)\t0.260911562139\n",
      "  (1, 31)\t0.260911562139\n",
      "  (1, 50)\t0.214985188511\n",
      "  (1, 118)\t0.260911562139\n",
      "  (1, 162)\t0.474299510549\n",
      "  (2, 0)\t0.104575924405\n",
      "  (2, 49)\t0.218578078103\n",
      "  (2, 30)\t0.379231221922\n",
      "  (2, 37)\t0.268526421121\n",
      "  (2, 31)\t0.268526421121\n",
      "  :\t:\n",
      "  (231, 0)\t0.117725418183\n",
      "  (231, 49)\t0.246062330281\n",
      "  (231, 50)\t0.249081096148\n",
      "  (231, 20)\t0.132660855952\n",
      "  (231, 109)\t0.364041096211\n",
      "  (231, 15)\t0.597255322555\n",
      "  (231, 119)\t0.597255322555\n",
      "  (232, 0)\t0.129681379704\n",
      "  (232, 20)\t0.146133631106\n",
      "  (232, 46)\t0.19319571623\n",
      "  (232, 71)\t0.190521778617\n",
      "  (232, 171)\t0.195925951382\n",
      "  (232, 86)\t0.539085550045\n",
      "  (232, 85)\t0.747799679078\n",
      "  (233, 0)\t0.0858627593619\n",
      "  (233, 20)\t0.0967558860878\n",
      "  (233, 46)\t0.127915953164\n",
      "  (233, 71)\t0.126145524268\n",
      "  (233, 171)\t0.129723656971\n",
      "  (233, 132)\t0.328041092144\n",
      "  (233, 33)\t0.376091247878\n",
      "  (233, 86)\t0.356931526829\n",
      "  (233, 84)\t0.365978071431\n",
      "  (233, 13)\t0.460307954005\n",
      "  (233, 103)\t0.460307954005\n",
      "0.914893617021\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Running the model and getting the vectorizer and the model to be used for prediction from TL function\n",
    "vec,mod =  TL()\n",
    "# URLs that we have to predict\n",
    "X_predict = ['https://faucetface.com/pages/stores']\n",
    "# Transforming the URLs\n",
    "X_predict = vec.transform(X_predict)\n",
    "# Predicting based on the model\n",
    "y_Predict = mod.predict(X_predict)\n",
    "# Printing the results\n",
    "print(y_Predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
